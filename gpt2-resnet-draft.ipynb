{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image captioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAVdSbFcF2CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czkwirJd3h2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "from collections import namedtuple\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSlmo5R3kbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_ZrjOWy398D",
        "colab_type": "code",
        "outputId": "8f6f1e6e-0017-4293-ec97-43fdac1f0994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "transform = transforms.Compose([\n",
        "     transforms.Resize((224, 224)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00xGTzcs0AkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, cnn, embed_size):\n",
        "        super().__init__()\n",
        "        self.cnn = cnn\n",
        "        self.embed_size = embed_size\n",
        "        _, last_module = list(cnn.named_children())[-1]\n",
        "        self.proj = nn.Linear(last_module.out_features, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return self.proj(x)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        self.cnn.eval()\n",
        "        self.proj.train(mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgrRF6xvQBxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = ('img', 'desc', 'pad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxjDrNtFah07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SpecialIds = namedtuple('speacil_ids', SPECIAL_TOKENS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51xuqPqz42bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Image2TextDescriptor(nn.Module):\n",
        "    def __init__(self, pretrained_model_text='gpt2', \n",
        "                     pretrained_model_image='resnet18'):\n",
        "        super().__init__()\n",
        "        self.tokenizer, self.special_ids = self._build_tokenizer(pretrained_model_text)\n",
        "        self.gpt2 = self._build_gpt2(len(self.tokenizer), pretrained_model_text).to(device)\n",
        "\n",
        "        self.img_tok_emb, self.desc_tok_emb = self.gpt2.transformer.wte(\n",
        "                    torch.tensor([self.special_ids.img, self.special_ids.desc]).to(device)\n",
        "        )\n",
        "        self.img_tok_emb.to(device)\n",
        "        self.desc_tok_emb.to(device)\n",
        "\n",
        "        cnn = self._build_cnn(pretrained_model_image).to(device)\n",
        "        self.embedding = Embedding(cnn, self.gpt2.config.hidden_size).to(device)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, img_data, text_tok_ids=None):\n",
        "        context_embeds = self.get_context_embeds(img_data).to(device)\n",
        "        labels_ids = None\n",
        "        if text_tok_ids:\n",
        "            text_tok_ids = torch.tensor(text_tok_ids).to(device)\n",
        "            text_embeds = self.gpt2.transformer.wte(text_tok_ids).to(device)\n",
        "            full_embeds = torch.cat((context_embeds, text_embeds), dim=1).to(device)\n",
        "            if self.training:\n",
        "                labels_ids = self.get_labels(\n",
        "                    full_embeds.shape[:-1],\n",
        "                    context_embeds.size(1),\n",
        "                    text_tok_ids\n",
        "                ).to(device)\n",
        "        else:\n",
        "            full_embeds = context_embeds\n",
        "        \n",
        "        out = self.gpt2(inputs_embeds=full_embeds, labels=labels_ids)        \n",
        "        return out\n",
        "\n",
        "    def get_context_embeds(self, img_data):\n",
        "        img_embeds = self.embedding(img_data)[:, None, :] # new shape: (batch_size, seq_len, embed_size)\n",
        "        context_embeds = torch.cat((self.img_tok_emb.expand_as(img_embeds), \n",
        "                                    img_embeds, \n",
        "                                    self.desc_tok_emb.expand_as(img_embeds)), dim=1)\n",
        "        return context_embeds\n",
        "\n",
        "    def get_labels(self, shape, context_len, tok_ids):\n",
        "        labels_ids = torch.empty(shape, dtype=torch.long).fill_(-100) # -100 is mask value for labels in hugginface models\n",
        "        mask = (tok_ids == self.special_ids.pad)\n",
        "        labels_ids[:, context_len:] = tok_ids.masked_fill(mask, -100)\n",
        "        print(label_ids)\n",
        "\n",
        "        return label_ids\n",
        "\n",
        "    def _build_tokenizer(self, pretrained_model):\n",
        "        special_dct = {t: f\"<{t.upper()}>\" for t in SPECIAL_TOKENS}\n",
        "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        tokenizer.add_special_tokens({'additional_special_tokens': list(special_dct.values())})\n",
        "        special_ids = SpecialIds(**{k: tokenizer.encode(v)[0] for k, v in special_dct.items()})\n",
        "        \n",
        "        return tokenizer, special_ids\n",
        "\n",
        "    def _build_gpt2(self, vocab_size, pretrained_model):\n",
        "        gpt2 = AutoModelWithLMHead.from_pretrained(pretrained_model)\n",
        "        gpt2.resize_token_embeddings(vocab_size)\n",
        "        \n",
        "        return gpt2\n",
        "\n",
        "    def _build_cnn(self, pretrained_model):\n",
        "        if pretrained_model == 'resnet18':\n",
        "            return models.resnet18(pretrained=True)\n",
        "        else:\n",
        "            raise ValueError(f'{pretrained_model} is not supported yet :(')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arq_P2cVWe5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "descriptor = Image2TextDescriptor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lxHyDVvKHxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, target = next(iter(trainloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIuLoBPyEam7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad(seq, max_len, pad_elem=special_ids.PAD):\n",
        "    return seq + [pad_elem] * max(0, max_len - len(seq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ru6HUwb58TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids_1 = tok.encode('The man is seeting on chair')\n",
        "ids_2 = tok.encode('The cat eat meet')\n",
        "text_ids = [ids_1, pad(ids_2, len(ids_1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6fUkFKLeDvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e571cc6d-04f4-4ae8-ebf4-0c6ff9fe73dc"
      },
      "source": [
        "len(text_ids[0])"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGc_I1MTeHPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fcaf3893-70c8-450b-e972-880c1d7e68d0"
      },
      "source": [
        "text_ids"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[464, 582, 318, 384, 13629, 319, 5118],\n",
              " [464, 3797, 4483, 1826, 50259, 50259, 50259]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO9NNFH5aElu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0a47b32a-0c9a-4fd4-adf3-1e87e3a063c5"
      },
      "source": [
        "out = descriptor(data.float().to(device), text_ids)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -100,  -100,  -100,   464,   582,   318,   384, 13629,   319,  5118],\n",
            "        [ -100,  -100,  -100,   464,  3797,  4483,  1826,  -100,  -100,  -100]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710DCK0mef9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "27399732-b440-497a-cdd1-869442ba578b"
      },
      "source": [
        "out[0]"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(82.0220, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0SBa0jveglh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}