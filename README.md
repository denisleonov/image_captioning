# image_captioning
HSE deep learning project

Основная суть нашего метода следующая. Наша модель состоит из двух частей -- энкодер и декодер. Энкодер отвечает за получение эмбеддинга картинки, это можно сделать с помощью предобученной сверточной сети, а декодер -- за преобразование эмбеддинга изображения в последовательность токенов слов.

В виде декодера в качестве бейзлайна мы взяли ячейку LSTM и на вход ей подавали эмбеддинг картинки, который по размеру совпадает с эмбеддингом токенов.

## Наши текущие рещультаты

### `DetectionToLSTM.ipynb`

Здесь в качестве декодера взята сеть FasteRCNN, в которой в качестве `backbone` модели используется ResNet-50-FPN, эмбединги получаются с помощью применения полносвязного слоя к последнему сверточному слою. Все это обучалось на датасете MS COCO. В конце ноутбука можно посмотреть предсказания для подписей в процессе обучения.

### `LamaLennyNotebook.ipynb`

Здесь Лиза пишет про свое :)

Основной моделью для декодера у нас будет трансформер -- на данный момент `GPT2`.

### `gpt2-resnet-draft.ipynb`

Денчик пишет про свое :)

## Планы на будущее

Для начала попробуем использовать несколько эмбеддингов с разных слоев в качестве префикса на вход `GPT2`. Кажется, что так модель сможет лучше выделять контекст из изображения, в этом ей поможет attention, который будет брать информацию не только с первого и единственного эмбеддинга картинки, а сразу с нескольких разных эмбеддингов.
