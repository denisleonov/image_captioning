# image_captioning
HSE deep learning project

Основная суть нашего метода следующая. Наша модель состоит из двух частей -- энкодер и декодер. Энкодер отвечает за получение эмбеддинга картинки, это можно сделать с помощью предобученной сверточной сети, а декодер -- за преобразование эмбеддинга изображения в последовательность токенов слов.

В виде декодера в качестве бейзлайна мы взяли ячейку LSTM и на вход ей подавали эмбеддинг картинки, который по размеру совпадает с эмбеддингом токенов.

## Наши текущие результаты

### `DetectionToLSTM.ipynb`

Здесь в качестве декодера взята сеть `FasteRCNN`, в которой в качестве `backbone` модели используется `ResNet-50-FPN`, эмбеддинги получаются с помощью применения полносвязного слоя к последнему сверточному слою. Все это обучалось на датасете MS COCO. В конце ноутбука можно посмотреть предсказания для подписей в процессе обучения.

### `SimpleCnnToLSTM.ipynb`

В сетке DenseNet121 (она выбрана из-за своего небольшого веса, может еще поменяется) убран слой классификатора и заменен на обычный линейный слой, который и будет выдавать эмбеддинги. Декодер - LSTM. Пока что запускалось только локально, поэтому cверточные слои DenseNet121 были заморожены и обучался только последний линейный слой. Обучение происходило на датасете Flickr8k.

Что планируется сделать в этой части в ближайшее время: разморозить все слои в DenseNet121 (как показала последняя домашняя работа по DL, обучать только последний линейный слой это не очент хорошая идея), посмотреть на то, какие подписи предсказывает модель (пока на обучении смотрелся только  loss). Еще планируется конечно дообучать модель=)

Основной моделью для декодера у нас будет трансформер -- на данный момент `GPT2`.

### `gpt2-resnet-draft.ipynb`

Денчик пишет про свое :)

## Планы на будущее

Для начала попробуем использовать несколько эмбеддингов с разных слоев в качестве префикса на вход `GPT2`. Кажется, что так модель сможет лучше выделять контекст из изображения, в этом ей поможет attention, который будет брать информацию не только с первого и единственного эмбеддинга картинки, а сразу с нескольких разных эмбеддингов.
